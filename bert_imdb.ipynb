{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57048a1e-a308-427b-acc8-a9d61ba7219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This practice is inspired by:\n",
    "# https://github.com/CSCfi/machine-learning-scripts/blob/master/notebooks/pytorch-imdb-bert.ipynb\n",
    "# https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31925662-b8b8-4509-a421-98efd5c66557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (TensorDataset, DataLoader,\n",
    "                              RandomSampler, SequentialSampler)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup as WarmupLinearSchedule\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    devicename = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314227b8-5705-48e4-beaa-8f68985fb743",
   "metadata": {},
   "source": [
    "<h1>IMDb Dataset</h1>\n",
    "\n",
    "<h4>IMDb dataset is a widely used dataset for natural language processing (NLP) and sentiment analysis tasks. It consists of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n",
    "<a href=\"https://ai.stanford.edu/~amaas/data/sentiment/\">https://ai.stanford.edu/~amaas/data/sentiment/</a></h4>\n",
    "<h4>We write our own dataset downloading and extraction process here.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696d177-bafd-4a13-a593-4eb3266e82a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tarfile\n",
    "import subprocess\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with open(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    \n",
    "    url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    fname = \"aclImdb.tar.gz\"\n",
    "\n",
    "    dataset = \"data/\"\n",
    "\n",
    "    # if downloaded:\n",
    "    if (os.path.exists(os.path.join(dataset, fname[:fname.find('.')], 'train_df.p')) and \n",
    "        os.path.exists(os.path.join(dataset, fname[:fname.find('.')], 'test_df.p')) ):\n",
    "\n",
    "        print(\"Dataset has already downloaded, loading catched dataset instead.\")\n",
    "        train_df = pd.read_pickle(os.path.join(dataset, fname[:fname.find('.')], 'train_df.p'))\n",
    "        test_df = pd.read_pickle(os.path.join(dataset, fname[:fname.find('.')], 'test_df.p'))\n",
    "\n",
    "    else:\n",
    "        print(\"Downloading...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(dataset, fname), 'wb') as f:\n",
    "                f.write(response.raw.read())\n",
    "        else:\n",
    "            raise Exception(\"Downloading error.\")\n",
    "        # untar\n",
    "        print(\"Extracting...\")\n",
    "        subprocess.call([\"tar\", \"xzf\", os.path.join(dataset, fname), \n",
    "                         \"-C\", dataset])\n",
    "        train_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
    "                                             \"aclImdb\", \"train\"))\n",
    "        test_df = load_dataset(os.path.join(os.path.dirname(dataset),\n",
    "                                            \"aclImdb\", \"test\"))\n",
    "        if os.path.exists(os.path.join(dataset, fname[:fname.find('.')], 'train_df.p')):\n",
    "            os.remove(os.path.join(dataset, fname[:fname.find('.')], 'train_df.p'))\n",
    "        if os.path.exists(os.path.join(dataset, fname[:fname.find('.')], 'test_df.p')):\n",
    "            os.remove(os.path.join(dataset, fname[:fname.find('.')], 'test_df.p'))\n",
    "        \n",
    "        train_df.to_pickle(\"data/aclImdb/train_df.p\")\n",
    "        test_df.to_pickle(\"data/aclImdb/test_df.p\")\n",
    "        print(\"Dataset is downloaded.\")\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545d6784-f468-450c-aae1-f5a08a15e238",
   "metadata": {},
   "source": [
    "<h4>Call the dataloading function to load the dataset to Pandas dataframes.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2399d7-5463-4fea-b84b-6c39af8bce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255d61d-546e-4545-96fd-8f9b0029ccd8",
   "metadata": {},
   "source": [
    "<h4>For this practice, we'll reduce the training data to make the demonstration more efficient.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a28866-83cb-48b4-9ea5-1329d56a22b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(3200, random_state=1)\n",
    "test_df = test_df.sample(3200, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f245bfa0-8aec-488a-9d2f-334611ded471",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's take a look of the first 10 samples in the train set.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3af93-07e5-44d9-8875-ac29f135fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('IMDB data loaded:')\n",
    "print('train:', train_df.shape)\n",
    "print('numbers of negative reviews vs positive reviews: \\n', \n",
    "      len(train_df[train_df.polarity == 0]),\n",
    "      \" vs. \",\n",
    "      len(train_df[train_df.polarity == 1]),\n",
    "     )\n",
    "print('test:', test_df.shape)\n",
    "print('numbers of negative reviews vs positive reviews: \\n', \n",
    "      len(test_df[test_df.polarity == 0]),\n",
    "      \" vs. \",\n",
    "      len(test_df[test_df.polarity == 1]),\n",
    "     )\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078e3a1-f686-4371-93bf-eacf20edc9ca",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's take a look of the data format in the dataset.\n",
    "</h4>\n",
    "<h5>The token [CLS] is a special token required by BERT at the beginning of the sentence.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef533f-eacb-493f-a022-f5e7aec955d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = {\n",
    "    0: 'negative',\n",
    "    1: 'positive',\n",
    "}\n",
    "sentences_train = train_df.sentence.values\n",
    "sentences_train = [\"[CLS] \" + s for s in sentences_train]\n",
    "\n",
    "sentences_test = test_df.sentence.values\n",
    "sentences_test = [\"[CLS] \" + s for s in sentences_test]\n",
    "\n",
    "labels_train = train_df.polarity.values\n",
    "labels_test  = test_df.polarity.values\n",
    "\n",
    "print (\"The first training sentence:\")\n",
    "print()\n",
    "print(sentences_train[0])\n",
    "print()\n",
    "print('LABEL:', class_names[labels_train[0]])\n",
    "print()\n",
    "print (\"The first testing sentence:\")\n",
    "print()\n",
    "print(sentences_test[0])\n",
    "print()\n",
    "print('LABEL:', class_names[labels_test[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867c6440-b77a-4d40-bd21-93c111563938",
   "metadata": {},
   "source": [
    "<h1>Data preparation</h1>\n",
    "\n",
    "<h4>IMDB dataset is also a perfectly balanced. Please note that we randomly reduced both train and test sets to 3,200 samples. Thus, the above sample counts for the train set are slightly biased.</h4>\n",
    "<h4>However, before using the BERT model, the text needs to be tokenized into a format it can understand. </h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194229a-7776-470f-bd44-400844babeba",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's get the tokenizer of the BERT model. Then, tokenize the dataset.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad0266-ffda-45b3-8738-63633d286699",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTMODEL='bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERTMODEL,\n",
    "                                          do_lower_case=True)\n",
    "\n",
    "tokenized_train = [tokenizer.tokenize(s) for s in sentences_train]\n",
    "tokenized_test  = [tokenizer.tokenize(s) for s in sentences_test]\n",
    "\n",
    "print (\"The full tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "337b41c6-8a3c-45ba-a275-32c946c12e72",
   "metadata": {},
   "source": [
    "<h4>BERT doesn’t understand raw text like humans do. Instead, it processes numerical representations of text. To bridge this gap, tokenization is necessary.</h4>\n",
    "<h4>The above will take a little time as the tokenization needs to go through every symbols and words in the dataset.</h4>\n",
    "<h4>\n",
    "While we are waiting, let's see a little more details about BERT.\n",
    "</h4> \n",
    "<h4>It needs the text input to have an ID and be splited by words and punctuations, so that the relationships between the words and punctuations can be learned.\n",
    "</h4>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b5/BERT_embeddings_01.png\" alt=\"BERT Encoder\">'\n",
    "<h4>\n",
    "    In addition, there is an input length limit for the BERT model. So, next step, we need to truncate the formated tokens.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0662a9-3d4a-4601-b679-8fc1b3cde613",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_TRAIN, MAX_LEN_TEST = 128, 512\n",
    "\n",
    "tokenized_train = [t[:(MAX_LEN_TRAIN-1)]+['SEP'] for t in tokenized_train]\n",
    "tokenized_test  = [t[:(MAX_LEN_TEST-1)]+['SEP'] for t in tokenized_test]\n",
    "\n",
    "print (\"The truncated tokenized first training sentence:\")\n",
    "print (tokenized_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8671a6a-d7c1-45e7-97ef-9537e629ac20",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's take a look into the IDs of the first training sentence.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7196c7-1581-4cbc-92f4-990d6fc65936",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_train = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_train]\n",
    "ids_train = np.array([np.pad(i, (0, MAX_LEN_TRAIN-len(i)), \n",
    "                             mode='constant') for i in ids_train])\n",
    "\n",
    "ids_test = [tokenizer.convert_tokens_to_ids(t) for t in tokenized_test]\n",
    "ids_test = np.array([np.pad(i, (0, MAX_LEN_TEST-len(i)), \n",
    "                            mode='constant') for i in ids_test])\n",
    "\n",
    "print (\"The indices of the training sentence:\")\n",
    "print (ids_train[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ec6f4-0aa6-498b-873a-7aac0b854349",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Why there are 0's in the IDs?\n",
    "</h4>\n",
    "<h4>\n",
    "    All inputs must be the same length. However, not all sentences are equally length — so the 0's are the padding to make the length uniform.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b250fc74-323e-4c52-bc77-be0460ca140c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    BERT uses self-attention, which means it learns to understand a sentence by using the sentence itself—predicting the masked parts based on the unmasked context. \n",
    "</h4>\n",
    "<h4>\n",
    "    Next, let’s create the attention masks for our dataset.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6597c5-ade4-4f88-b0f9-37aea421a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "amasks_train, amasks_test = [], []\n",
    "\n",
    "for seq in ids_train:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_train.append(seq_mask)\n",
    "    \n",
    "for seq in ids_test:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  amasks_test.append(seq_mask)\n",
    "\n",
    "print (\"The masked training sentence:\")\n",
    "print (amasks_train[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad5c4a5-2b42-4d2b-81c4-0b55528fe9bd",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's split the training set to create a validation set.\n",
    "</h4>\n",
    "<h4>\n",
    "    Why?\n",
    "</h4>\n",
    "<h4>\n",
    "    A validation set is used to evaluate the model during training, without touching the test set. It helps you answer:\n",
    "    <br><br>\n",
    "\"How well is my model learning, and when should I stop training?\"\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e78838f-97a1-4698-b5e4-1a892f74d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_inputs, validation_inputs, \n",
    " train_labels, validation_labels) = train_test_split(ids_train, labels_train, \n",
    "                                                     random_state=42,\n",
    "                                                     test_size=0.1)\n",
    "(train_masks, validation_masks, \n",
    " _, _) = train_test_split(amasks_train, ids_train,\n",
    "                          random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cedf4-3f26-4d50-bd12-90d92039edec",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    We have the data prepared now. Let's create the dataloader for all these sets to prepare for the next step -- training.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc9d0c7-1908-49d3-84c3-96a2a7a77fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all sets to tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks  = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks  = torch.tensor(validation_masks)\n",
    "test_inputs = torch.tensor(ids_test)\n",
    "test_labels = torch.tensor(labels_test)\n",
    "test_masks  = torch.tensor(amasks_test)\n",
    "\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('Train: ', end=\"\")\n",
    "train_data = TensorDataset(train_inputs, train_masks,\n",
    "                           train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, \n",
    "                              batch_size=BATCH_SIZE)\n",
    "print(len(train_data), 'reviews')\n",
    "\n",
    "print('Validation: ', end=\"\")\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks,\n",
    "                                validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,\n",
    "                                   sampler=validation_sampler,\n",
    "                                   batch_size=BATCH_SIZE)\n",
    "print(len(validation_data), 'reviews')\n",
    "\n",
    "print('Test: ', end=\"\")\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler,\n",
    "                             batch_size=BATCH_SIZE)\n",
    "print(len(test_data), 'reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bb017-1a47-4b49-856c-8cff57fe4596",
   "metadata": {},
   "source": [
    "<h1>Use a pre-trained BERT model.</h1>\n",
    "<h4>\n",
    "    Instead of training from scratch, we start with a pre-trained BERT model that has already learned a lot about the structure and meaning of language.\n",
    "</h4>\n",
    "<h5>\n",
    "    For example, the model here (google-bert) was pretrained on BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers). By using a pre-trained model, we don’t need to train from scratch for it to understand grammar, context, and word relationships. Instead, we can simply fine-tune BERT on our own smaller dataset for a specific task.\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308a227-d519-44f8-b26a-c4ba9dbb8f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTMODEL='bert-base-uncased'\n",
    "class_names = {\n",
    "    0: 'negative',\n",
    "    1: 'positive',\n",
    "}\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERTMODEL, \n",
    "                                                      num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b654433-ce0b-4140-ab6d-752c91d98239",
   "metadata": {},
   "source": [
    "<h4>Move the model to GPU to prepare for training</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985c076-6782-4559-baaf-c65306360350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322fe6e-1afd-4c26-8ccd-1954ff7226d2",
   "metadata": {},
   "source": [
    "<h1>Train the model.</h1>\n",
    "<h4>\n",
    "    Let's configure the hyperparameters frist.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34883dc1-8c2a-4c68-a991-77d56afe225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "BATCH_SIZE = 32\n",
    "# Number of training epochs -- one full pass through the entire train set by the model\n",
    "EPOCHS = 4\n",
    "# Learning rate -- how much model learns for every batch of training\n",
    "LR = 2e-5\n",
    "# weight decay for the Adam optimizer\n",
    "WEIGHT_DECAY = 0.01 \n",
    "\n",
    "WARMUP_STEPS =int(0.2*len(train_dataloader))\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=LR, eps=1e-8)\n",
    "scheduler = WarmupLinearSchedule(optimizer, num_warmup_steps=WARMUP_STEPS,\n",
    "                                 num_training_steps=len(train_dataloader)*EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b8dde-ecac-4e15-9f25-104829e94d8e",
   "metadata": {},
   "source": [
    "<h4>Let's define the function for training and the function for validation.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561510c-a737-4efc-9f15-b166cc0536a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, loss_vector=None, log_interval=15):\n",
    "  # Set model to training mode\n",
    "  model.train()\n",
    "  \n",
    "  # Loop over each batch from the training set\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "    # Copy data to GPU if needed\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    \n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Zero gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs[0]\n",
    "    if loss_vector is not None:\n",
    "        loss_vector.append(loss.item())\n",
    "        \n",
    "    # start a TensorBoard summary writer to log testing\n",
    "    writer = SummaryWriter(\"log/txt_cls/imdb_experiment\")\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), ((epoch-1)*len(train_dataloader))+step )\n",
    "    writer.close()\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    scheduler.step()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, step * len(b_input_ids),\n",
    "                len(train_dataloader.dataset),\n",
    "                100. * step / len(train_dataloader), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f45f28-7e4d-4a08-9ba4-0d9b4aee1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader, epoch, validation = True):\n",
    "    model.eval()\n",
    "    \n",
    "    n_correct, n_all = 0, 0\n",
    "    \n",
    "    for batch in loader:\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      #print(b_input_ids.shape)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "    \n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "      labels = b_labels.to('cpu').numpy()\n",
    "      n_correct += np.sum(predictions == labels)\n",
    "      n_all += len(labels)\n",
    "\n",
    "    if validation:\n",
    "        # start a TensorBoard summary writer to log testing\n",
    "        writer = SummaryWriter(\"log/txt_cls/imdb_experiment\")\n",
    "        writer.add_scalar(\"Accuracy/validation\", n_correct/n_all, epoch)\n",
    "    else:\n",
    "        # start a TensorBoard summary writer to log testing\n",
    "        writer = SummaryWriter(\"log/txt_cls/imdb_experiment\")\n",
    "        writer.add_scalar(\"Accuracy/test\", n_correct/n_all, epoch)\n",
    "    writer.close()\n",
    "    print('Accuracy: [{}/{}] {:.4f}\\n'.format(n_correct, n_all, n_correct/n_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56eac13-a472-4db0-8616-318d5054490f",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Train the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3ed27-f6bf-4bb6-a20c-38f10119d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lossv = []\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, train_lossv)\n",
    "    print('\\nValidation set:')\n",
    "    evaluate(validation_dataloader, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397d36c-eeca-4c5c-9896-b6f5b2a68a8f",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Training performance.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdf159-e879-4084-a15d-3040a8965f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Training loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.plot(train_lossv, label='original')\n",
    "plt.plot(np.convolve(train_lossv, np.ones(101), 'same') / 101, label='averaged')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff60b1-e3de-496b-aeb0-fe2ee87ec247",
   "metadata": {},
   "source": [
    "<h1>Evaluate the trained model.</h1>\n",
    "<h4>\n",
    "    Noticed the test set was never used or even mentioned in any place?\n",
    "</h4>\n",
    "<h4>It's <b><u>crucial</u></b> that the testing set remains completely unseen throughout the training and validation phases to avoid data contamination for accurate evaluation.​</h4>\n",
    "<h4>There are various metrics available to assess the effectiveness of a trained model. Selection often depends on the study case. Here, we evaluate only overall accuracy for simplicity and demonstration purposes.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd5d67-8fb9-4328-a87e-d9e9e46c4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The evaluation function can also be used by the testing set. \n",
    "# But the testing set samples should alway reamin unseen during trianing.\n",
    "print('Test set:')\n",
    "evaluate(test_dataloader, EPOCHS, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7ae68-7ca6-4b5d-9aed-4580343eb358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcc-ai-ml-workshop-2025",
   "language": "python",
   "name": "hcc-ai-ml-workshop-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
