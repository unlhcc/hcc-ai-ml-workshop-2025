{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ed625f-2ddf-408b-9b22-777a918ab5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This practice is inspired by:\n",
    "# https://medium.com/@golnaz.hosseini/beginner-tutorial-image-classification-using-pytorch-63f30dcc071c\n",
    "# https://www.digitalocean.com/community/tutorials/alexnet-pytorch\n",
    "# https://medium.com/thecyphy/train-cnn-model-with-pytorch-21dafb918f48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624b138-c00f-487b-8ae5-455e25dc1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72115d-f5b4-4960-8b12-a8000ced7219",
   "metadata": {},
   "source": [
    "<h1>CIFAR-10 Dataset</h1>\n",
    "\n",
    "<h4>CIFAR-10 dataset consist of 60,000 color images in 10 distinct classes. It has been used to benchmark AI and machine learning models for image classification.\n",
    "<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">https://www.cs.toronto.edu/~kriz/cifar.html</a></h4>\n",
    "<h4>PyTorch provides convenient download and dataloading functions.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf83ed8-7ca3-4f7e-af42-f08bec41f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of images per training/testing batch.\n",
    "# Here, we use a convenient nubmer for visualize the dataset. \n",
    "# This number is a subject to be optimized in actual experiments.\n",
    "batch_size = 32\n",
    "# Data preprocessing techniques. Here, we block the normalization for convenient of visualize the dataset.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "# Downloading trainset of CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# Downloading testset of CIFAR-10 dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd321454-3b1b-49c2-aa3b-82f6ad851998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building train data loader for the dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Building test data loader for the dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77f19e-9e8a-47cb-846c-685a4d88ebff",
   "metadata": {},
   "source": [
    "<h4>Let's print the 10 class names of CIFAR-10.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24513c-44e6-4f10-80fe-52e6b6a4f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Inspect distinct classes of CIFAR-10\n",
    "# List of all classes\n",
    "classes = trainset.classes\n",
    "# List of labels for all images\n",
    "labels = trainset.targets\n",
    "# Count images per class\n",
    "label_counts = Counter(labels)\n",
    "\n",
    "print(\"ID\\t#image\\tlabel name\")\n",
    "for idx, it in enumerate(classes):\n",
    "    print(str(idx)+'\\t'+str(label_counts[idx])+'\\t'+'\\033[1m'+str(it)+'\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ecee6-6ec0-4a98-87f9-82783dc1038c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Visualize the first image for each class in the trainset\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff8e79-eb77-4360-9ac5-dc9e86ed3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to store first image for each class\n",
    "first_images = []\n",
    "# Loop over dataset to find the first image of each class\n",
    "for img, label in trainset:\n",
    "    if label not in first_images:\n",
    "        first_images.append([img, label])\n",
    "    if len(first_images) == len(classes):\n",
    "        break\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(1, len(classes), figsize=(18,8))\n",
    "# Draw iamges\n",
    "for i in range(len(classes)):\n",
    "    im, lbl = first_images[i]\n",
    "    ax[i].imshow(np.transpose(im.numpy(), (1,2,0)), interpolation='bilinear')\n",
    "    ax[i].set_title(f'{classes[lbl]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff800ba-1e94-4fae-b871-07273a0527f7",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Why are images bulary?\n",
    "    <h5>Due to historical reasons: CIFAR-10 images are very small — only 32x32 pixels — which is low resolution by modern standards. <br>\n",
    "        This fits for early convolutional neural network study for faster experiments.\n",
    "    </h5>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55228a-15ee-4941-9699-f596065edadf",
   "metadata": {},
   "source": [
    "<h1>Data preparation</h1>\n",
    "\n",
    "<h4>CIFAR-10 dataset is a processed and perfectly balanced.</h4>\n",
    "<h4>We need normalization for equal importance across channels and learned features.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82f317-a34d-4997-9acd-6ad9c6ef730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing techniques. Here, we block the normalization for convenient of visualize the dataset.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "# Building train data loader for the dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Building test data loader for the dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3845f5-0172-43f2-9d2d-ecbaa0d1dc7c",
   "metadata": {},
   "source": [
    "<h4>Let's see the difference after normalization.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257eb95-2114-47f6-8138-c3e249c1b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map to store first image for each class\n",
    "first_images = []\n",
    "# Loop over dataset to find the first image of each class\n",
    "for img, label in trainset:\n",
    "    if label not in first_images:\n",
    "        first_images.append([img, label])\n",
    "    if len(first_images) == len(classes):\n",
    "        break\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(1, len(classes), figsize=(18,8))\n",
    "# Draw iamges\n",
    "for i in range(len(classes)):\n",
    "    im, lbl = first_images[i]\n",
    "    ax[i].imshow(np.transpose(im.numpy(), (1,2,0)), interpolation='bilinear')\n",
    "    ax[i].set_title(f'{classes[lbl]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1308fa1-65fc-4f50-8030-a74be1ed835b",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Compared with original images:\n",
    "<h5>\n",
    "    - Image has negative values, so the warning shows images has values in invalid range. <br>\n",
    "    - This is for computer to see, compared with for human to see.\n",
    "</h5>\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ef15e-1dcd-4b0d-b099-a54ee8eb8c1f",
   "metadata": {},
   "source": [
    "<h1>Build a convlutional neural network model.</h1>\n",
    "<h4>\n",
    "    Let's build a custom network consist of 4-block convolutional layers followed by linear layers to generate predicaiton.\n",
    "</h4>\n",
    "<h5>\n",
    "    Each block contains:\n",
    "    <ul>\n",
    "        <li>Conv2d (convolutional layer)</li>\n",
    "        <li>BatchNorm2d (reduces internal covariate shift)</li>\n",
    "        <li>ReLU (add non-linearity)</li>\n",
    "        <li>MaxPool2d (preserve only important feature)</li>\n",
    "    </ul>\n",
    "    After 4 blocks, we use a classifier with:\n",
    "    <ul>\n",
    "    <li>2 fully connected (Linear) layers with dropout layer to control overfitting</li>\n",
    "    <li>Final output layer for 10-class classification (CIFAR-10)</li>\n",
    "    </ul>\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b04c8a-77da-44b4-ac53-eb7de71e5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN from scratch\n",
    "# PyTorch's base class for a machine learning model: torch.nn.Module\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2))\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 2560),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(2560, 2560),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(2560, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ee78e-80e0-42b6-b863-0337356b41fe",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Let's print the model architecture.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db702f4b-a172-4eb8-8dd8-babdc099cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b1d720-4684-43d1-a4c4-e427863c9041",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Not a fan of this format. Let's use TensorBoard to print a pretty format.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6709732-0787-4f42-bb50-5248a4ed2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "dummy_input = torch.randn(1, 3, 32, 32)  # CIFAR-10 shape\n",
    "with SummaryWriter(\"log/img_cls/custom_cnn\") as writer:\n",
    "    writer.add_graph(model, dummy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788afe22-e177-4215-b9a3-b0582b97b99c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Hyperparameters are a core part of model selection in machine learning.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0aad1-0c01-495c-b35b-43be95dd18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = CustomNet().to(device)\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 320         # often limited by available memory\n",
    "num_epochs = 20          # number of training epochs -- one full pass through the entire train set by the model\n",
    "learning_rate = 0.005   # how much model learns for every batch of training\n",
    "criterion = nn.CrossEntropyLoss()                 # Cross Entropy Loss is often used for classification task\n",
    "optimizer = torch.optim.SGD(model.parameters(),   # Gradient decent algorithm used for updated the model weights -- learning\n",
    "                            lr=learning_rate, \n",
    "                            weight_decay = 0.005, \n",
    "                            momentum = 0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef33dcb-3dcd-42bd-a712-c9cb43f32d28",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Apply updated hyperparameters to the data loader.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6521f31e-060b-4573-a8a2-067955f7283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building train data loader for the dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Building test data loader for the dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e74f5-bc23-40e0-a420-cfa7f1ab01ea",
   "metadata": {},
   "source": [
    "<h1>Train the model.</h1>\n",
    "<h4>\n",
    "    Let's first see if GPU available for us.\n",
    "</h4>\n",
    "<h4>In PyTorch, the string 'cuda' refers to using NVIDIA GPUs via the CUDA backend, otherwise the device is CPU</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6940a-2e4a-4447-9a38-e8a299df8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU device, or CPU if GPU is not detected\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a33467-fc34-4abd-9d0b-e870d0198e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log training\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment\")\n",
    "\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    writer.add_scalar(\"Accuracy/train\", correct/total, epoch)\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2cb8a-99a6-4655-adc0-f4eca585ec05",
   "metadata": {},
   "source": [
    "<h1>Evaluate the trained model.</h1>\n",
    "<h4>\n",
    "    Noticed the test set was never used or even mentioned in any place?\n",
    "</h4>\n",
    "<h4>It's <b><u>crucial</u></b> that the testing set remains completely unseen throughout the training and validation phases to avoid data contamination for accurate evaluation.​</h4>\n",
    "<h4>There are various metrics available to assess the effectiveness of a trained model. Selection often depends on the study case. Here, we evaluate only overall accuracy for simplicity and demonstration purposes.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22617093-ce77-43f7-8360-bdd9b315631c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log testing\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    writer.add_scalar(\"Accuracy/test\", correct/total, num_epochs)\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8543cb-7dfd-49d9-a858-9dac6c8060a8",
   "metadata": {},
   "source": [
    "<h1>Wonder how hyperparameters affect the training?</h1>\n",
    "<h4>\n",
    "    Larger batch size:\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6032ac2-5bab-4673-9430-dc7722782e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = CustomNet().to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 10000       # often limited by available memory\n",
    "num_epochs = 20          # number of training epochs -- one full pass through the entire train set by the model\n",
    "learning_rate = 0.005   # how much model learns for every batch of training\n",
    "criterion = nn.CrossEntropyLoss()                 # Cross Entropy Loss is often used for classification task\n",
    "optimizer = torch.optim.SGD(model.parameters(),   # Gradient decent algorithm used for updated the model weights -- learning\n",
    "                            lr=learning_rate, \n",
    "                            weight_decay = 0.005, \n",
    "                            momentum = 0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8ddd8-8c4c-416d-aca2-797ba65c9726",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Apply updated hyperparameters to the data loader.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37520626-1ad3-453c-9392-d68afe778f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building train data loader for the dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Building test data loader for the dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6181063-eebc-4656-bbcc-642a72101116",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Train the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fc6d2b-df29-4beb-8556-cecd48be5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log training\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment_larger_batch_size\")\n",
    "\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    writer.add_scalar(\"Accuracy/train\", correct/total, epoch)\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27871f01-9430-4ea8-bc71-1c394253b060",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Evaluate the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f7b3d-ddaa-4fc1-aa4f-ba7c0029d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log testing\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment_larger_batch_size\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    writer.add_scalar(\"Accuracy/test\", correct/total, num_epochs)\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c8baa-91f3-4013-99a0-f08a175f278c",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Larger learning rate:\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390ca47-6796-41de-9904-f1bb37abf876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = CustomNet().to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 320         # often limited by available memory\n",
    "num_epochs = 20          # number of training epochs -- one full pass through the entire train set by the model\n",
    "learning_rate = 0.05   # how much model learns for every batch of training\n",
    "criterion = nn.CrossEntropyLoss()                 # Cross Entropy Loss is often used for classification task\n",
    "optimizer = torch.optim.SGD(model.parameters(),   # Gradient decent algorithm used for updated the model weights -- learning\n",
    "                            lr=learning_rate, \n",
    "                            weight_decay = 0.005, \n",
    "                            momentum = 0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0707f4c8-649a-4344-843f-9c2374a634d4",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Apply updated hyperparameters to the data loader.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90af475-d2f6-4abf-bbd8-51fa8b05d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building train data loader for the dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Building test data loader for the dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca102f4-39b0-4ab7-baf6-1b51119da5d7",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Train the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27777d-acff-4d26-8740-911d5bcf2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log training\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment_larger_learning_rate\")\n",
    "\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    writer.add_scalar(\"Accuracy/train\", correct/total, epoch)\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea3c843-37e3-47fa-afe9-da638079fd71",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Evaluate the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4f750-c630-427d-89da-7686166200ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log testing\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment_larger_learning_rate\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    writer.add_scalar(\"Accuracy/test\", correct/total, num_epochs)\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359991ea-7f36-4f3b-b1bc-f8bce39328d8",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    More training epochs:\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980bc8f-53c7-40ef-a9a3-4175693e360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model\n",
    "model = CustomNet().to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 320         # often limited by available memory\n",
    "num_epochs = 40          # number of training epochs -- one full pass through the entire train set by the model\n",
    "learning_rate = 0.005   # how much model learns for every batch of training\n",
    "criterion = nn.CrossEntropyLoss()                 # Cross Entropy Loss is often used for classification task\n",
    "optimizer = torch.optim.SGD(model.parameters(),   # Gradient decent algorithm used for updated the model weights -- learning\n",
    "                            lr=learning_rate, \n",
    "                            weight_decay = 0.005, \n",
    "                            momentum = 0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fe1ab-3b59-48bf-a604-89aef0fcea9a",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Apply updated hyperparameters to the data loader.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f67d40-167a-4509-86db-62d7e936651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building train data loader for the dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Building test data loader for the dataset\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d4681-eb01-4df8-a2b5-e7cec227d5be",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Train the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cd5931-8452-4d60-a0a3-a6c984da3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log training\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment/more_training_epoch\")\n",
    "\n",
    "total_step = len(trainloader)\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(trainloader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    writer.add_scalar(\"Accuracy/train\", correct/total, epoch)\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), epoch)\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318dbf9-c41f-438d-897e-27c22b8517fa",
   "metadata": {},
   "source": [
    "<h4>\n",
    "    Evaluate the model.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ac0b6-5152-4f76-9284-ce6fe6443a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a TensorBoard summary writer to log testing\n",
    "writer = SummaryWriter(\"log/img_cls/cifar10_experiment_more_training_epoch\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "        \n",
    "    writer.add_scalar(\"Accuracy/test\", correct/total, 20)\n",
    "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a3997d-9a60-4d62-b1fa-a11fed1c862e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcc-ai-ml-workshop-2025",
   "language": "python",
   "name": "hcc-ai-ml-workshop-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
